{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stanford TF Course Assi_1_LogReg_MNIST.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/mahmoudSalim/stanford-tensorflow-tutorials/blob/master/Stanford_TF_Course_Assi_1_LogReg_MNIST.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "acPjUeOYr8qX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gzip\n",
        "import shutil\n",
        "import struct\n",
        "import urllib\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def huber_loss(labels, predictions, delta=14.0):\n",
        "    residual = tf.abs(labels - predictions)\n",
        "    def f1(): return 0.5 * tf.square(residual)\n",
        "    def f2(): return delta * residual - 0.5 * tf.square(delta)\n",
        "    return tf.cond(residual < delta, f1, f2)\n",
        "\n",
        "def safe_mkdir(path):\n",
        "    \"\"\" Create a directory if there isn't one already. \"\"\"\n",
        "    try:\n",
        "        os.mkdir(path)\n",
        "    except OSError:\n",
        "        pass\n",
        "\n",
        "def read_birth_life_data(filename):\n",
        "    \"\"\"\n",
        "    Read in birth_life_2010.txt and return:\n",
        "    data in the form of NumPy array\n",
        "    n_samples: number of samples\n",
        "    \"\"\"\n",
        "    text = open(filename, 'r').readlines()[1:]\n",
        "    data = [line[:-1].split('\\t') for line in text]\n",
        "    births = [float(line[1]) for line in data]\n",
        "    lifes = [float(line[2]) for line in data]\n",
        "    data = list(zip(births, lifes))\n",
        "    n_samples = len(data)\n",
        "    data = np.asarray(data, dtype=np.float32)\n",
        "    return data, n_samples\n",
        "\n",
        "def download_one_file(download_url, \n",
        "                    local_dest, \n",
        "                    expected_byte=None, \n",
        "                    unzip_and_remove=False):\n",
        "    \"\"\" \n",
        "    Download the file from download_url into local_dest\n",
        "    if the file doesn't already exists.\n",
        "    If expected_byte is provided, check if \n",
        "    the downloaded file has the same number of bytes.\n",
        "    If unzip_and_remove is True, unzip the file and remove the zip file\n",
        "    \"\"\"\n",
        "    if os.path.exists(local_dest) or os.path.exists(local_dest[:-3]):\n",
        "        print('%s already exists' %local_dest)\n",
        "    else:\n",
        "        print('Downloading %s' %download_url)\n",
        "        local_file, _ = urllib.request.urlretrieve(download_url, local_dest)\n",
        "        file_stat = os.stat(local_dest)\n",
        "        if expected_byte:\n",
        "            if file_stat.st_size == expected_byte:\n",
        "                print('Successfully downloaded %s' %local_dest)\n",
        "                if unzip_and_remove:\n",
        "                    with gzip.open(local_dest, 'rb') as f_in, open(local_dest[:-3],'wb') as f_out:\n",
        "                        shutil.copyfileobj(f_in, f_out)\n",
        "                    os.remove(local_dest)\n",
        "            else:\n",
        "                print('The downloaded file has unexpected number of bytes')\n",
        "\n",
        "def download_mnist(path):\n",
        "    \"\"\" \n",
        "    Download and unzip the dataset mnist if it's not already downloaded \n",
        "    Download from http://yann.lecun.com/exdb/mnist\n",
        "    \"\"\"\n",
        "    safe_mkdir(path)\n",
        "    url = 'http://yann.lecun.com/exdb/mnist'\n",
        "    filenames = ['train-images-idx3-ubyte.gz',\n",
        "                'train-labels-idx1-ubyte.gz',\n",
        "                't10k-images-idx3-ubyte.gz',\n",
        "                't10k-labels-idx1-ubyte.gz']\n",
        "    expected_bytes = [9912422, 28881, 1648877, 4542]\n",
        "\n",
        "    for filename, byte in zip(filenames, expected_bytes):\n",
        "        download_url = os.path.join(url, filename)\n",
        "        local_dest = os.path.join(path, filename)\n",
        "        download_one_file(download_url, local_dest, byte, True)\n",
        "\n",
        "def parse_data(path, dataset, flatten):\n",
        "    if dataset != 'train' and dataset != 't10k':\n",
        "        raise NameError('dataset must be train or t10k')\n",
        "\n",
        "    label_file = os.path.join(path, dataset + '-labels-idx1-ubyte')\n",
        "    with open(label_file, 'rb') as file:\n",
        "        _, num = struct.unpack(\">II\", file.read(8))\n",
        "        labels = np.fromfile(file, dtype=np.int8) #int8\n",
        "        new_labels = np.zeros((num, 10))\n",
        "        new_labels[np.arange(num), labels] = 1\n",
        "    \n",
        "    img_file = os.path.join(path, dataset + '-images-idx3-ubyte')\n",
        "    with open(img_file, 'rb') as file:\n",
        "        _, num, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
        "        imgs = np.fromfile(file, dtype=np.uint8).reshape(num, rows, cols) #uint8\n",
        "        imgs = imgs.astype(np.float32) / 255.0\n",
        "        if flatten:\n",
        "            imgs = imgs.reshape([num, -1])\n",
        "\n",
        "    return imgs, new_labels\n",
        "\n",
        "def read_mnist(path, flatten=True, num_train=55000):\n",
        "    \"\"\"\n",
        "    Read in the mnist dataset, given that the data is stored in path\n",
        "    Return two tuples of numpy arrays\n",
        "    ((train_imgs, train_labels), (test_imgs, test_labels))\n",
        "    \"\"\"\n",
        "    imgs, labels = parse_data(path, 'train', flatten)\n",
        "    indices = np.random.permutation(labels.shape[0])\n",
        "    train_idx, val_idx = indices[:num_train], indices[num_train:]\n",
        "    train_img, train_labels = imgs[train_idx, :], labels[train_idx, :]\n",
        "    val_img, val_labels = imgs[val_idx, :], labels[val_idx, :]\n",
        "    test = parse_data(path, 't10k', flatten)\n",
        "    return (train_img, train_labels), (val_img, val_labels), test\n",
        "\n",
        "def get_mnist_dataset(batch_size):\n",
        "    # Step 1: Read in data\n",
        "    mnist_folder = 'data/mnist'\n",
        "    download_mnist(mnist_folder)\n",
        "    train, val, test = read_mnist(mnist_folder, flatten=False)\n",
        "\n",
        "    # Step 2: Create datasets and iterator\n",
        "    train_data = tf.data.Dataset.from_tensor_slices(train)\n",
        "    train_data = train_data.shuffle(10000) # if you want to shuffle your data\n",
        "    train_data = train_data.batch(batch_size)\n",
        "\n",
        "    test_data = tf.data.Dataset.from_tensor_slices(test)\n",
        "    test_data = test_data.batch(batch_size)\n",
        "\n",
        "    return train_data, test_data\n",
        "    \n",
        "def show(image):\n",
        "    \"\"\"\n",
        "    Render a given numpy.uint8 2D array of pixel data.\n",
        "    \"\"\"\n",
        "    plt.imshow(image, cmap='gray')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HNUotZVotoEp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p /home/data/mnist/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jI2mI-tjpl6I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "f09bd61d-ac1c-4f0c-c0a0-688d0fd04005"
      },
      "cell_type": "code",
      "source": [
        "#@title Default title text\n",
        "\"\"\" Starter code for simple logistic regression model for MNIST\n",
        "with tf.data module\n",
        "MNIST dataset: yann.lecun.com/exdb/mnist/\n",
        "Created by Chip Huyen (chiphuyen@cs.stanford.edu)\n",
        "CS20: \"TensorFlow for Deep Learning Research\"\n",
        "cs20.stanford.edu\n",
        "Lecture 03\n",
        "\"\"\"\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import time\n",
        "\n",
        "#import utils\n",
        "\n",
        "# Define paramaters for the model\n",
        "learning_rate = 0.01\n",
        "batch_size = 128\n",
        "n_epochs = 100\n",
        "n_train = 60000\n",
        "n_test = 10000\n",
        "\n",
        "# Step 1: Read in data\n",
        "\n",
        "\n",
        "mnist_folder = '/home/data/mnist'\n",
        "download_mnist(mnist_folder)\n",
        "train, val, test = read_mnist(mnist_folder, flatten=True)\n",
        "\n",
        "# Step 2: Create datasets and iterator\n",
        "# create training Dataset and batch it\n",
        "train_data = tf.data.Dataset.from_tensor_slices(train)\n",
        "train_data = train_data.shuffle(10000) # if you want to shuffle your data\n",
        "train_data = train_data.batch(batch_size)\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded /home/data/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Successfully downloaded /home/data/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded /home/data/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Successfully downloaded /home/data/mnist/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "f7C5jjKyu1lm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V7oFVlv4qS8_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create testing Dataset and batch it\n",
        "test_data = tf.data.Dataset.from_tensor_slices(test)\n",
        "test_data = test_data.shuffle(1000)\n",
        "test_data = test_data.batch(batch_size)\n",
        "\n",
        "#############################\n",
        "########## TO DO ############\n",
        "#############################\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LMhJVbKpqWL2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create one iterator and initialize it with different datasets\n",
        "iterator = tf.data.Iterator.from_structure(train_data.output_types, \n",
        "                                           train_data.output_shapes)\n",
        "img, label = iterator.get_next()\n",
        "\n",
        "train_init = iterator.make_initializer(train_data)\t# initializer for train_data\n",
        "test_init = iterator.make_initializer(test_data)\t# initializer for train_data\n",
        "\n",
        "# Step 3: create weights and bias\n",
        "# w is initialized to random variables with mean of 0, stddev of 0.01\n",
        "# b is initialized to 0\n",
        "# shape of w depends on the dimension of X and Y so that Y = tf.matmul(X, w)\n",
        "# shape of b depends on Y\n",
        "\n",
        "#X = tf.placeholder(dtype=tf.float32, shape=[batch_size, 784])\n",
        "#Y = tf.placeholder(dtype=tf.float32, shape=[batch_size, 10])\n",
        "\n",
        "w = tf.Variable(tf.random_normal(mean=0, stddev=0.01, shape=[784, 10]))\n",
        "b = tf.Variable(tf.zeros(shape=[1, 10]))\n",
        "\n",
        "#w, b = tf.Variable(tf.random_normal(mean=0, stddev=0.01, shape=[img.shape[1], label.shape[1]])), tf.Variable(tf.zeros(shape=label.shape))\n",
        "#############################\n",
        "########## TO DO ############\n",
        "#############################\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cQyT4MR0qaTt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Step 4: build model\n",
        "# the model that returns the logits.\n",
        "# this logits will be later passed through softmax layer\n",
        "logits = tf.matmul(img, w) + b\n",
        "#############################\n",
        "########## TO DO ############\n",
        "#############################\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7zdQHPMcqdPm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Step 5: define loss function\n",
        "# use cross entropy of softmax of logits as the loss function\n",
        "entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=label)\n",
        "loss = tf.reduce_mean(entropy)\n",
        "#############################\n",
        "########## TO DO ############\n",
        "#############################\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cFI64WfZqfpt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Step 6: define optimizer\n",
        "# using Adamn Optimizer with pre-defined learning rate to minimize loss\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
        "#############################\n",
        "########## TO DO ############\n",
        "#############################\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VxVQs2ykqh5Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1785
        },
        "outputId": "735b1f69-32a9-45ca-a884-5c5b4d9b4cea"
      },
      "cell_type": "code",
      "source": [
        "# Step 7: calculate accuracy with test set\n",
        "preds = tf.nn.softmax(logits)\n",
        "correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(label, 1))\n",
        "accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
        "\n",
        "writer = tf.summary.FileWriter('./graphs/logreg', tf.get_default_graph())\n",
        "with tf.Session() as sess:\n",
        "   \n",
        "    start_time = time.time()\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    # train the model n_epochs times\n",
        "    for i in range(n_epochs): \t\n",
        "        sess.run(train_init)\t# drawing samples from train_data\n",
        "        total_loss = 0\n",
        "        n_batches = 0\n",
        "        try:\n",
        "            while True:\n",
        "                _, l = sess.run([optimizer, loss])\n",
        "                total_loss += l\n",
        "                n_batches += 1\n",
        "        except tf.errors.OutOfRangeError:\n",
        "            pass\n",
        "        print('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
        "    print('Total time: {0} seconds'.format(time.time() - start_time))\n",
        "\n",
        "    # test the model\n",
        "    sess.run(test_init)\t\t\t# drawing samples from test_data\n",
        "    total_correct_preds = 0\n",
        "    try:\n",
        "        while True:\n",
        "            accuracy_batch = sess.run(accuracy)\n",
        "            total_correct_preds += accuracy_batch\n",
        "    except tf.errors.OutOfRangeError:\n",
        "        pass\n",
        "\n",
        "    print('Accuracy {0}'.format(total_correct_preds/n_test))\n",
        "writer.close()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average loss epoch 0: 0.36414208065631776\n",
            "Average loss epoch 1: 0.2940820986794871\n",
            "Average loss epoch 2: 0.2816020430867062\n",
            "Average loss epoch 3: 0.2768041499305603\n",
            "Average loss epoch 4: 0.2728691224268703\n",
            "Average loss epoch 5: 0.2688182164070218\n",
            "Average loss epoch 6: 0.26921509020896844\n",
            "Average loss epoch 7: 0.2662141303510167\n",
            "Average loss epoch 8: 0.2641593280573224\n",
            "Average loss epoch 9: 0.26568590788993723\n",
            "Average loss epoch 10: 0.2647259741848291\n",
            "Average loss epoch 11: 0.26152696957768395\n",
            "Average loss epoch 12: 0.258115794319053\n",
            "Average loss epoch 13: 0.25720747068177824\n",
            "Average loss epoch 14: 0.2598081780900789\n",
            "Average loss epoch 15: 0.2561981923011846\n",
            "Average loss epoch 16: 0.2573577952246333\n",
            "Average loss epoch 17: 0.2539565304164277\n",
            "Average loss epoch 18: 0.25674585048542464\n",
            "Average loss epoch 19: 0.2519130958720695\n",
            "Average loss epoch 20: 0.2538898273782675\n",
            "Average loss epoch 21: 0.2541149542601996\n",
            "Average loss epoch 22: 0.25211830914540345\n",
            "Average loss epoch 23: 0.2521684065461159\n",
            "Average loss epoch 24: 0.251699090593083\n",
            "Average loss epoch 25: 0.252910714579183\n",
            "Average loss epoch 26: 0.25199917402378347\n",
            "Average loss epoch 27: 0.25190784931182864\n",
            "Average loss epoch 28: 0.25206749165473985\n",
            "Average loss epoch 29: 0.2501710182185783\n",
            "Average loss epoch 30: 0.2483581014150797\n",
            "Average loss epoch 31: 0.24900634417007136\n",
            "Average loss epoch 32: 0.2510108813643456\n",
            "Average loss epoch 33: 0.2514014492027981\n",
            "Average loss epoch 34: 0.2483592773939288\n",
            "Average loss epoch 35: 0.24944105610944503\n",
            "Average loss epoch 36: 0.24930561775731486\n",
            "Average loss epoch 37: 0.25008593067353546\n",
            "Average loss epoch 38: 0.25106568040196287\n",
            "Average loss epoch 39: 0.24758444307155386\n",
            "Average loss epoch 40: 0.24962548454140507\n",
            "Average loss epoch 41: 0.24853923480178033\n",
            "Average loss epoch 42: 0.2466485768729864\n",
            "Average loss epoch 43: 0.24857045892019605\n",
            "Average loss epoch 44: 0.24746611726491952\n",
            "Average loss epoch 45: 0.24720721693579542\n",
            "Average loss epoch 46: 0.24580505440054937\n",
            "Average loss epoch 47: 0.24611350084806596\n",
            "Average loss epoch 48: 0.24591215041487716\n",
            "Average loss epoch 49: 0.24678323909986852\n",
            "Average loss epoch 50: 0.24705911682442178\n",
            "Average loss epoch 51: 0.24605727920005488\n",
            "Average loss epoch 52: 0.24548252256110656\n",
            "Average loss epoch 53: 0.24738284713307093\n",
            "Average loss epoch 54: 0.2467657583224219\n",
            "Average loss epoch 55: 0.24429799398017485\n",
            "Average loss epoch 56: 0.2463281750679016\n",
            "Average loss epoch 57: 0.24621313834606215\n",
            "Average loss epoch 58: 0.2432996668701255\n",
            "Average loss epoch 59: 0.24572506323456764\n",
            "Average loss epoch 60: 0.24540805645113767\n",
            "Average loss epoch 61: 0.24744072984470877\n",
            "Average loss epoch 62: 0.24129905893012535\n",
            "Average loss epoch 63: 0.2433511963590633\n",
            "Average loss epoch 64: 0.2455532543361187\n",
            "Average loss epoch 65: 0.24255592537134193\n",
            "Average loss epoch 66: 0.24817840247306713\n",
            "Average loss epoch 67: 0.24517055074142854\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Average loss epoch 68: 0.2453836320704499\n",
            "Average loss epoch 69: 0.24318492315536322\n",
            "Average loss epoch 70: 0.24137048636411512\n",
            "Average loss epoch 71: 0.24390938876326693\n",
            "Average loss epoch 72: 0.24379069870987605\n",
            "Average loss epoch 73: 0.24334337666284206\n",
            "Average loss epoch 74: 0.24501731593248455\n",
            "Average loss epoch 75: 0.24432663149958433\n",
            "Average loss epoch 76: 0.2448380328541578\n",
            "Average loss epoch 77: 0.2457387316538844\n",
            "Average loss epoch 78: 0.2456878859289857\n",
            "Average loss epoch 79: 0.2428288457005523\n",
            "Average loss epoch 80: 0.2448053181517956\n",
            "Average loss epoch 81: 0.243366892202649\n",
            "Average loss epoch 82: 0.2417978424319001\n",
            "Average loss epoch 83: 0.24268014074064964\n",
            "Average loss epoch 84: 0.24279898483046267\n",
            "Average loss epoch 85: 0.24648421273328538\n",
            "Average loss epoch 86: 0.24227125681070394\n",
            "Average loss epoch 87: 0.2446879244821016\n",
            "Average loss epoch 88: 0.24434675274546755\n",
            "Average loss epoch 89: 0.24185810456442278\n",
            "Average loss epoch 90: 0.2429957128541414\n",
            "Average loss epoch 91: 0.24456505236584086\n",
            "Average loss epoch 92: 0.24425988992632822\n",
            "Average loss epoch 93: 0.2440355014142602\n",
            "Average loss epoch 94: 0.2419252541522647\n",
            "Average loss epoch 95: 0.24296777674625086\n",
            "Average loss epoch 96: 0.2409869707081207\n",
            "Average loss epoch 97: 0.24168192042514336\n",
            "Average loss epoch 98: 0.24216447692624357\n",
            "Average loss epoch 99: 0.2437772951846899\n",
            "Total time: 128.26044011116028 seconds\n",
            "Accuracy 0.9169\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gbtjlGZwrwus",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1f215eb0-cd1b-42c3-f2af-b10b803d06a9"
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "logreg\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ty-KoZaTrzzm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "LOG_DIR = '/content/datalab/graphs/logreg'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rJkbR9j6OzcH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "815a9082-2706-4ed9-eb1a-8fe6c3643e86"
      },
      "cell_type": "code",
      "source": [
        "! wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "! unzip ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-04-15 16:59:39--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 23.23.215.144, 23.21.132.31, 23.21.140.88, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|23.23.215.144|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5363700 (5.1M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]   5.11M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2018-04-15 16:59:40 (40.9 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [5363700/5363700]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tnHRzkw9P3Dh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "get_ipython().system_raw('./ngrok http 6006 &')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ydhT5QrVP7rR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "058e188a-bac8-48d1-fa9c-d5ab7016d703"
      },
      "cell_type": "code",
      "source": [
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "http://4844db15.ngrok.io\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uJolaie-P_BO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}